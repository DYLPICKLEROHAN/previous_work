---
title: "ASSESSMENT 3 â€“ CASE STUDY AND DATA ANALYSIS"
output: word_document
---

```{r}
library("tidyverse")
library("dataMeta")
library("caret")
library("skimr")
library("ggcorrplot")
library("tidymodels")
library("vip")
```

## Summary to CEO

Hello Mr Tuke,

Regarding the predicted sales of 'The Fatal Empire'. With the information we have about it's title length, number of platforms released on, specific platform of release, game genre, year of release, and the sales figures of Japan, Europe, and other countries, our random forest model predicts sales of 1.92 million copies.

Taking the data as it is, and assuming the historical data is still representative, we can be over 90% sure in the model's accuracy. That being said, there are a few areas for improvement, but I'll cc you in on the manager's report were that will be discussed further.

Regards, Dylan

## Managers report

### Managers report

1.  **Aim and Hypothesis:**

The purpose of this report is to outline the process undertaken to predict the North American sales figure of the PS4 release of 'The Fatal Empire' using data available online via kaggle that was originally sourced from www.vgchatz.com. The data provided included information regarding a games:

-    Name,

-   Year of release

-   Platform of release

-   Game genre

-   Sales figures of North America,

-   Sales figures of Japan

-   Sales figures of Europe

-   Sales figures for the total of all other countries.

From this data, two additional pieces of information were acquired, the title length, as well as the number of platforms a game had been released on as it was hypothesised that human behaviour relating to these two factors would be informative (Title length may influence a potential customers likely hood of picking up a game in store, and more platforms influences word-of-mouth advertising as well as the community available to a game).

Two types of predictive models were trialed, A Lasso regression and a random forest. Their accuracy was measured and function carefully examined.

1.  **Data Cleaning**

In this phase:

Summary of what has been accomplished below:

-   A duplicate was removed
-   An approach to verify the Name and Publisher values was provided
-   The Year variable was properly formatted and NA values addressed. 270 NA values were reduced to 133 by setting them equal to the average year for rows with the same title. Five of the remaining titles had a year in their title which made for a good approximation of its release year. The remaining values were given the mean year for rows with the same Platform. This was considered a better approximation on a case by case basis than simply setting them all equal to the global mean year.
-   Some of the sales figures were representing weekly sales, these have been converted to annual figures where obvious. Of those, six were for US/American sales which would relate to the Other_Sales value. As there is no way to easily multiply this column (because it contains multiple countries and not just America) by 52.14, they have been left as is with the knowledge that this will influence the accuracy of the model
-   Two incorrect Platform values (as indicated in the Name column) were corrected

Issues Identified but not fixed:

-   As mentioned above, the US weekly sales rows remain an issue

-   I've not changed the names to reflect the corrections made (this remains a source of error for the title length variable)

-   There is no simple way to merge data pertaining to the same game because there is no linking variable/ID. And even if there were, it would need to be done individually and manually as some values would be written over in the process.

1.  **Exploratory Data Analysis (EDA)**

In this phase, the information most relevant to predicting North American sales was identified. After careful examination of all variables it was found that the title length, Platform, Year, Genre, all Sales figures, and the number of platforms a game had been released on were all considered informative for the prediction of North American sales.

These assertions were identified through simple bivariate analysis, principle component analysis, and parallel coordinate plots.. The parallel coordinate plot found that there was a relationship between platform and region; Nintendo platforms generally selling better in Japan and 'Sony' platforms generally selling better outside of Japan. A similar trend could be found for some publishers, but it's likely just that some publishers produce games for Nintendo platforms/ Japanese audiences. The principle component analysis found that the numerical variables contained the most amount of variance in the data, but that is only saying that no specific category contained much variation on its own. The principle component analysis offered the best evidence at this stage that the additional variables would make good predictors.

Their importances were also confirmed after the fact through variable importance analyses.

1.  **Data Processing**

The data was modified as required. In this case, that meant transforming sales figures (by log(x+1)) and normalizing all countable variables. Platform and genre were also treated in a manner that allowed a computer to interpret each category.

The data was then split into training and testing sets. This is done so that the model can be tested on data that was not intrinsically part of its production.

1.  **Model Fitting and Model Evaluation**

The model fitting stage involves building four models and measuring their accuracy through out-of-bag error measures (this involves comparing

). A fifth model was created in an attempt to improve/ verify the variable importance measures obtained by the best model, but it provided no benefit in terms of prediction accuracy.

The models created were:

1.  LASSO regression model - Initial variables

2.  LASSO regression model - initial variables + (Title length, platforms available to game)

3.  Random forest model - Initial variables

4.  Random forest model - initial variables + (Title length, platforms available to game)

5.  Random forest model - initial variables + (Title length, platforms available to game) + preprocessing steps to reduce categories and correlation

There are a few assumptions involved in the LASSO regression model that were verified in this section. However the relevance of the LASSO models was lacking. Of the above list, it was the fourth model that had the best accuracy as it was found to have the highest r-squared value and the lowest route mean squared

## Statisticians report

A detailed analysis of the data intended for a fellow statistician. The analysis should include any code, figures and tables appropriately captioned. This should thoroughly detail your analysis process such that it is independently reproducible by another statistician. [85 marks]

## Data Clean

### Part 1 - Import and skim

```{r}
# Importing data
vgsales <- read.csv("vgsales.csv")
head(vgsales,6)
```

```{r}
# Initial skim of Data
skim(vgsales)
# No missing values here
```

The data has nine columns, with 16,598 rows. There are currently five character variables and 4 numeric variables. The variables include the name of the game (Name), the platform the game can be played on (Platform), the year the game was released (Year), the genre of the game (Genre), the publisher of the game (Publisher), as well as the number of copies sold in North America, Europe, Japan, and the total for all remaining countries (NA_Sales, EU_Sales, JP_Sales, Other_Sales, respectively). No missing values were made apparent [in this step]{.underline}.

## Data Clean

### Part 2 - A Good Clean and Tidy

Summary of what has been accomplished below:

-   A duplicate was removed
-   An approach to verify the Name and Publisher values was provided
-   The Year variable was properly formatted and NA values addressed. 270 NA values were reduced to 133 by setting them equal to the average year for rows with the same title. Five of the remaining titles had a year in their title which made for a good approximation of its release year. The remaining values were given the mean year for rows with the same Platform. This was considered a better approximation on a case by case basis than simply setting them all equal to the global mean year.
-   A curiosity regarding Wii Sport was examined and validated
-   Some of the sales figures were representing weekly sales, these have been converted to annual figures where obvious. Of those, six were for US/American sales which would relate to the Other_Sales value. As we cannot simply multiply this column (because it contains multiple countries and not just America) by 52, they have been left as is with the knowledge that this will influence the accuracy of the model
-   Two incorrect Platform values were corrected

Issues Identified but not fixed:

-   As mentioned above, the US weekly sales rows remain an issue

-   I've not changed the names to reflect the corrections made (because Name is not a predictor)

-   I could not find a simple way to merge data pertaining to the same game because there is no linking variable/ID. And even if there were, I would still need to go through each manually as some values would be written over in the process

```{r}
# Check for Duplicates
filter(vgsales, duplicated(vgsales) == TRUE)
filter(vgsales, Name == "Wii de Asobu: Metroid Prime")
clean <- distinct(vgsales)
# One duplicate removed

# Check for other mistakes/inconsistencies:
# Name check:
name_vgsales <- clean %>%
  group_by(Name) %>%
  tally(sort=TRUE)        # currently 11,492 Game titles
# Need to compare values against a Master list

# Platform check:
platform_vgsales <- clean %>%
  group_by(Platform) %>%
  tally(sort=TRUE)        # 31 Platforms 
head(platform_vgsales,5) 
# Clear

# Publisher check
publishers_vgsales <- clean %>%
  group_by(Publisher) %>%
  tally(sort=TRUE)        # 579 Publishers
head(publishers_vgsales,5)
# Need to compare values against Master list

      # Check 579 publishers against list found on wiki and at:            https://www.kaggle.com/datasets/andreshg/videogamescompaniesregions?resource=download) for quick spell-check I create a Master list to compare Publisher values against: 
dev1 <- read.csv("dev1.csv")
dev2 <- read.csv("dev2.csv")
wiki1 <- read_csv("table-2.csv")
dev1_publishers <- dev1$Developer
dev2_publishers <- dev2$Developer
wiki_publishers <- wiki1$Publisher
publisher_check <-  c(dev1_publishers, dev2_publishers, wiki_publishers)
publisher_check <- unique(publisher_check)
length(publisher_check) # 1535 Publishers to check against
publisher_check_df <- data.frame(matrix(unlist(publisher_check),
                                        nrow=length(publisher_check),
                                        byrow=TRUE))
      # Comparing lists
publishers_vgsales$Publisher %in% publisher_check_df$matrix.unlist.publisher_check...nrow...length.publisher_check...

# With more resources you could verify from a 'master list', mine is incomplete and I have no way of knowing if it has errors itself. But its a proof of concept as to how you could go about checking the Qualitative variables.

# Year Check:
unique(clean$Year)
clean <- clean %>%
  mutate(Year = na_if(Year, "N/A"), Year = na_if(Year, ""), Year = na_if(Year, " ") )
clean$Year[is.nan(clean$Year)]<-NA
sum(is.na(clean$Year))
clean$Year <- as.numeric(clean$Year)
# Has 270 NA values and is now numeric

# Could just give the average if a game is released  on multiple platforms 
na_rows <- filter(clean, is.na(Year))
na_titles <- c(na_rows$Name)
# If a game is released on multiple platforms, let NA equal the average of the year 
for(i in na_titles){
  if(nrow(filter(clean, grepl(i, Name))) > 1){
    clean[clean$Name == i, "Year"] <- round(mean(filter(clean, grepl(i, Name))[,3], na.rm = TRUE),0)
  }
}
filter(clean, is.na(Year))
# Now only 133 NA values

# If the year is in the title, that's a good indication's likely to be within a year of the true value. These 5 titles below only appear in the data once, so we can alter by Name:
name_indicates_year <- filter(clean, grepl(" 20", Name))
Name_to_Year <- filter(name_indicates_year, !grepl("", Year))
clean[clean$Name == "wwe Smackdown vs. Raw 2006", "Year"] <- 2006
clean[clean$Name == "NFL GameDay 2003", "Year"] <- 2003
clean[clean$Name == "Tour de France 2011", "Year"] <- 2011
clean[clean$Name == "Sega Rally 2006", "Year"] <- 2006
clean[clean$Name == "Football Manager 2007", "Year"] <- 2006
filter(clean, is.na(Year))
# Now only 128 Na values

# The most accurate substitution for the remaining values would be the mean value for the year for games on that platform type... this may take a second
na_key <- group_by(clean, Platform) %>%
    summarize(m = round(mean(Year, na.rm=TRUE))) # Average year for each platform
# Make a new row for 'mean year by platform', then back-fill with ifelse statement
clean <- clean %>%
  left_join(na_key, by = "Platform")
clean$Year <- ifelse(is.na(clean$Year), clean$m, clean$Year)
clean <- select(clean, -m)
head(filter(clean, is.na(Year)))
head(clean)
```

```{r}
# Genre Check:
unique(clean$Genre)
genre_vgsales <- clean %>%
  group_by(Genre) %>%
  tally(sort=TRUE) 
# All clear

# Sales Checks:
max(clean$NA_Sales)
max(clean$EU_Sales)
max(clean$JP_Sales)
max(clean$Other_Sales)
head(filter(clean, NA_Sales <0 | NA_Sales > 20 ))
wii_sport <- filter(clean, NA_Sales <0 | NA_Sales > 40 )
# Wikipedia confirms Wii Sport actually did have 82 million copies sold by 2017
# Clear

# Is total world wide sales equal to the sum of all sales?
## filter(clean, (NA_Sales + EU_Sales + JP_Sales + Other_Sales) != Global_Sales)
# Apparently we're not using the Global column seen on kaggle in this assignment...

# Identified a re-occurring issue in sales:
dplyr::filter(clean, grepl("weekly", Name))
dplyr::filter(clean, grepl("Weekly", Name))
  # 20 rows report only weekly sales figures (multiply by 52)

# Altering all weekly Japanese sales figures to be annual
    # Creating list to sort
weekly_to_annual_all_rows <- filter(clean, grepl("weekly", Name) | grepl("Weekly", Name))
weekly_to_annual_jp_rows <- filter(weekly_to_annual_all_rows,
                                   grepl("JP", Name) | grepl("jp", Name)| grepl("Jp", Name))
weekly_to_annual_titles <- c(weekly_to_annual_jp_rows$Name)
    # looping through 
for(i in weekly_to_annual_titles){
  clean[clean$Name == i, "JP_Sales"] <- clean[clean$Name == i, "JP_Sales"]*52.14 # weeks per year  
}

# Investigating these other "weekly sales" figures
filter(weekly_to_annual_all_rows, !grepl("JP", Name) & !grepl("jp", Name)& !grepl("Jp", Name))
dplyr::filter(clean, grepl("Tony Hawk's American Wasteland", Name)) # Unclear
dplyr::filter(clean, grepl("NBA Live 06", Name)) # Unclear
dplyr::filter(clean, grepl("Ratchet & Clank: Up Your Arsenal", Name)) # Unclear
dplyr::filter(clean, grepl("Midnight Club 3", Name)) # Unclear
dplyr::filter(clean, grepl("Pokemon Mystery Dungeon: Red", Name)) # Unclear 
dplyr::filter(clean, grepl("The Urbz: Sims In the City", Name)) # Unclear
# Doesn't appear to be marks for fixing this...
```

```{r}
# As the name suggests, PS2 should be PS
filter(clean, grepl("wrong", Name))
clean[clean$Name=="Pachi-Slot Teiou: Golgo 13 Las Vegas (JP sales, but wrong system)", "Platform"] <- "PS"
  # Will leave it in, but unsure if this was even sold globally
clean[clean$Name=="Lunar 2: Eternal Blue(sales, but wrong system)", "Platform"] <- "SCD" # in 1994, the platform should be sega CD
filter(clean, grepl("wrong", Name))

# Identified a re-occurring issue in name:
multirow <- dplyr::filter(clean, grepl("sales", Name))
  # Some of the observations have been split into two rows.
multirow <- filter(multirow, !grepl("all region sales", Name))
multirow <- filter(multirow, !grepl("All region sales", Name))
multirow <- filter(multirow, !grepl("All Region sales", Name))
multirow <- filter(multirow, !grepl("All Region Sales", Name))
multirow <- filter(multirow, !grepl("all regions sales", Name))
multirow <- filter(multirow, !grepl("wrong system", Name))
  # 132 problematic titles that represent at least as many rows but likely more.
  # They each need to be assessed and bound to one row, or in the case of American/US/us sales, removed. But this is an enormous undertaking, for so few marks so....

```

## EDA

### Part 1 - Initial variable inspections

An exploratory data analysis was essential because it provided an opportunity to confirm the validity of our data and ensure it made sense. It would be pointless creating a model from inaccurate or non-nonsensical data. Depending on the model type selected, this phase generally assists in determining what transformations (if any) may be necessary and clearly identifies important characteristics/trends in each variable as well as relationships between variables.

```{r}
# Univariate analysis - Year
  # Histogram
ggplot(clean, aes(x = Year)) +
  geom_histogram(fill="grey", colour ="black") +
  ggtitle(str_wrap("Figure 1.2:The multimodal, left-skewed nature of the data with known Year values", 70) ) +
  xlab("Year") +
  ylab("Frequency") +
  theme_minimal()

  # Boxplot
ggplot(clean, aes(x = Year)) +
  geom_boxplot(fill = "grey") +
  ggtitle(str_wrap("Figure 1.1: 50% of the data has a Year value between 2003 and 2010, data prior 1993 appear to be considered outliers", 70 )) +
  xlab("Year") +
  ylab("") +
  theme_minimal()

  # Summary statistics
summary(clean$Year)
```

**Year**

The data sits on a domain of 1980 to 2020, with the interquartie range from 2003 to 2010. It is multi-modal with very few games listed prior to the mid-90's or in the last few years; suggesting that data collection over the entire period has been very inconsistent. The left skew is not what one might expect to see when game development and play has been on the rise over the last few years. This data has been sourced from a public website, perhaps it has seen a decline in users since 2010 resulting in less frequent additions.

The data appears to be a poor representation of historical trends. As copies sold in North America is the outcome variable and not the dollar value, we could ignore the year entirely. However, this leads to a bit of a dangerous assumption. It assumes that wages have grown with inflation and the same proportion of the market can afford to buy a copy no matter the year. It also assumes population sizes (customer population size specifically) have been consistent. Essentially, it removes the 'timeline' element and the relationships observed in time (more on this later).

```{r}
# Univariate analysis - NA_Sales
  # Histogram
ggplot(clean, aes(x = NA_Sales)) +
  geom_histogram(fill="lightblue", colour ="blue") +
  ggtitle(str_wrap("Figure 2.1: The right-skewed nature of the North american sales data", 70 )) +
  xlab("North American sales (in millions)") +
  ylab("Frequency") +
  theme_minimal()

  # Boxplot
ggplot(clean, aes(x = NA_Sales)) +
  geom_boxplot(fill = "lightblue") +
  ggtitle(str_wrap("Figure 2.2: 50% of games sold less than 250,000 copies in North America, \nand anything larger than around 500,000 copies was considered an outlier",70)) +
  xlab("North American sales (in millions)") +
  ylab("") +
  scale_x_continuous(limits = c(0, 1)) +
  theme_minimal()

  # Summary statistics
summary(clean$NA_Sales)
```

**NA_Sales**

The NA_Sales variable had a domain of 0 to 41.49 representing the number of copies sold in millions. The interquartile range was from 0 to 0.24 million. The mean number of copies sold was 0.2642 million, but the median was only 0.08 million. The data has a uni-modal shape with a clear right skew when looking at the histogram. The boxplot makes it more apparent that there are many small modes resulting from outliers across the domain.

```{r}
# Univariate analysis - JP_Sales
  # Histogram
ggplot(clean, aes(x = JP_Sales)) +
  geom_histogram(fill="green", colour ="black") +
  ggtitle(str_wrap("Figure 3.1: The right-skewed nature of the Japanese sales data", 70) ) +
  xlab("Japanese sales (in millions)") +
  ylab("Frequency") +
  theme_minimal()

  # Boxplot
ggplot(clean, aes(x = JP_Sales)) +
  geom_boxplot(fill = "green") +
  ggtitle(str_wrap("Figure 3.2: 50% of games sold less than 12,500  copies in Japan, and anything larger than \napproximately 25,000 copies was considered an outlier", 70)) +
  xlab("Japanese sales (in millions)") +
  ylab("") +
  scale_x_continuous(limits = c(0, .1)) +
  theme_minimal()

  # Summary statistics
summary(clean$JP_Sales)
filter(clean, JP_Sales>108)
```

**JP_Sales**

JP_Sales had a domain from 0 to 14.08 million copies, (this changed because I converted a weekly sales figure to an annual sales figure in the data cleaning phase). The Median value was 0.00, while the mean value was 0.08 million. Similar to North america, the data has a right skew with a dominant mode with many small modes that represent outliers as observed in the boxplot. The median value of 0 indicates that many of the games were not sold in Japan.

```{r}
#Univariate analysis - EU_Sales
  # Histogram
ggplot(clean, aes(x = EU_Sales)) +
  geom_histogram(fill = "red", colour ="black") +
  ggtitle(str_wrap("Figure 4.1: The right-skewed nature of the European sales data", 70)) +
  xlab("European sales (in millions)") +
  ylab("Frequency") +
  theme_minimal()

  # Boxplot
ggplot(clean, aes(x = EU_Sales)) +
  geom_boxplot(fill = "red") +
  ggtitle(str_wrap("Figure 4.2: 50% of games sold less than 30,000 copies in Europe, and anything larger than around 65,000 copies was considered an outlier", 70)) +
  xlab("European sales (in millions)") +
  ylab("") +
  scale_x_continuous(limits = c(0, .1)) +
  theme_minimal()

  # Summary Statistics
 summary(clean$EU_Sales)
```

**EU_Sales**

The EU_Sales data had a domain of 0 to 29.02 million. The median value was 0.02 million, while the mean was 0.15 million. Much like the other sales figures, this too has a dominant mode and a right skew with several smaller modes scattered across the domain in positions identified as outliers.

```{r}
# Univariate analysis - Other_Sales
  # Histogram
ggplot(clean, aes(x = Other_Sales)) +
  geom_histogram(fill = "purple", colour ="black") +
  ggtitle(str_wrap("Figure 5.1: The right-skewed nature of the Other sales data", 70 ) ) +
  xlab("Sales in other countries (in millions)") +
  ylab("Frequency") +
  theme_minimal()
  
  # Boxplot
ggplot(clean, aes(x = Other_Sales)) +
  geom_boxplot(fill = "purple") +
  ggtitle(str_wrap("Figure 5.2: Over 50% of games sold less than 25,000 copies in Other countries, and anything larger than around 50,000 copies was considered an outlier", 70 )) +
  xlab("Sales in other countries (in millions)") +
  ylab("") +
  scale_x_continuous(limits = c(0, .1)) +
  theme_minimal()

  # Summary statistics
summary(clean$Other_Sales)
```

**Other_Sales**

The Other_Sales data had a domain of 0 to 10.57 million. Its mean value was 0.05 million while it's median was 0.01 million. And it's the same story once more, dominant mode, right skew, several outliers.

## EDA

### Part 2 - Improving the Information Exchange between Data and Model

I expect the three most important predictors will be JP_Sales, EU_Sales, and Other_Sales because they represent the response to a game released in a given year on a given Platform of a given Genre; in a sense these variables contain latent information representing the other variables. While cultures, languages and preferences differ around the globe, a game worth buying in one part of the world is most likely worth buying in another:

```{r}
# Visualizing the relationship between NA_Sales against all sales variables
  # Comparison of JP_Sales, EU_Sales, and Other_Sales relationships with NA_Sales
sales_stats <- clean %>%
  pivot_longer(cols = c(EU_Sales, JP_Sales, Other_Sales)) 
library("ggpubr")
ggplot(sales_stats, aes(x = NA_Sales, y = value, colour = name)) +
  geom_point() +
  geom_smooth(method=lm, se=FALSE) +
  ggtitle(str_wrap("Figure 6.1: The positive relationship between historical North American sales and those observed in Europe, Japan, and other countries", 70))+
  xlab("North American Sales (in millions")+
  ylab("Sales (in millions)") +
  theme_dark()

  # NA_sales and EU_Sales
ggplot(clean, aes(x = NA_Sales, y = EU_Sales)) +
  geom_point(colour = '#F8766D', show.legend = FALSE) +
  geom_smooth(colour = '#F8766D', method = lm, se=FALSE, show.legend = FALSE) +
  ggtitle(str_wrap("Figure 6.2: The positive relationshi between historical North American and European sales", 70))+
  xlab("North American Sales (in millions")+
  ylab("European Sales (in millions)") +
  theme_dark() +
  ggpubr::stat_cor(aes(label = ..rr.label..), color = "red", geom = "label")

  # NA_sales and JP_Sales  
ggplot(clean, aes(x = NA_Sales, y = JP_Sales)) +
  geom_point(colour = '#00BA38', show.legend = FALSE) +
  geom_smooth(colour = '#00BA38', method=lm, se=FALSE, show.legend = FALSE) +
  ggtitle(str_wrap("Figure 6.3: The positive relationship between historical North American and Japan sales", 70))+
  xlab("North American Sales (in millions")+
  ylab("Japanese Sales (in millions)") +
  theme_dark() +
  ggpubr::stat_cor(aes(label = ..rr.label..), color = "red", geom = "label")

  # NA_sales and Other_Sales
ggplot(clean, aes(x = NA_Sales, y = Other_Sales)) +
  geom_point(colour = '#619CFF', show.legend = FALSE) +
  geom_smooth(colour = '#619CFF', method=lm, se=FALSE, show.legend = FALSE) +
  ggtitle(str_wrap("Figure 6.4: The positive relationshi between historical North American and all Other countries' sales", 70))+
  xlab("North American Sales (in millions")+
  ylab("All Other Sales (in millions)") +
  theme_dark() +
  ggpubr::stat_cor(aes(label = ..rr.label..), color = "red", geom = "label")
```

**Key predictors: EU_Sales, JP_Sales, and Other_Sales**

There does appear to be a positive correlation between these sales figures and NA_Sales. European_Sales has a moderate positive relation, perhaps due to cultural similarities as touched on briefly above. The relationship between JP_Sales is weakly positive. When a game sells well in North America, it doesn't appear to do as well in Japan, which could be partially related to cultural differences but also to the amount of customers in each country. Perhaps there are just more people to sell to in North America. Other_Sales follows a similar trend to that of JP_Sales, but it has a stronger correlation.

The right skew in the data should be addressed as a LASSO model is essentially at it's heart still a linear model. Linear models make four key assumptions:

1.  linearity

2.  Normality

3.  Homoscedasticity

4.  Independence

In the case of a LASSO model specifically though, each term in the otherwise linear model equation is given a penalty that relates to it's importance. Essentially weighing each variable in the equation by importance, with a few being weighed by zero and effectiely removed. When it comes to the assumptions of A LASSO linear model it is far simpler:

1.  linearity - a straight line is still the best model

2.  Sparcity - only a small number of variables may be relevant

3.  The Irrepresentable condition - the important variables are unrelated to the unimportant variables

4.  The errors must have a finite variance and a mean of zero, but not necessarily be normally distributed <https://click.endnote.com/viewer?doi=10.9734%2Fbjmcs%2F2016%2F29533&token=WzMzOTQxOTgsIjEwLjk3MzQvYmptY3MvMjAxNi8yOTUzMyJd.NYzJY0PZUBb7vphORpVLYuubCcc>,

The sales figures were transformed by log(x+1) in the modelling phase:

```{r, fig.width=6,fig.height=5}
sales_stats <- clean %>%
  pivot_longer(cols = c(EU_Sales, JP_Sales, Other_Sales)) %>%
  mutate(value=log(value+1))

ggplot(sales_stats, aes(x = NA_Sales, y = value, colour = name)) +
  geom_point() +
  geom_smooth(method=lm, se=FALSE) +
  ggtitle(str_wrap("Figure 6.5: The positive relationship between historical North American sales and those observed in Europe, Japan, and other countries after each variable has been transformed by log(x+1) \n(See correlations in figure 20)", 70))+
  xlab("The log of (North American Sales (in millions) + 1)") +
  ylab("The log of (Sales (in millions) + 1") +
  theme_dark()
```

**Improvement of Information exchange in other variables:**

**Name** - The [title length]{.underline}, [Title language]{.underline}, or [franchise indicators]{.underline} (such as ':', '-', 'II','2', reoccurring 'prefix' strings, etc.) is probably more informative than the exact String. The language indicates a proportion of the world that can effortlessly play the game and understand the title, and signs of a franchise/sequel indicate a pre-existing fan-base that will help drive sales. Longer titles might be less enticing to consumers; a trend observable in the data. The trend becomes less obvious when you include all the games, but when you take a subset (such as games that sell up to 5 million copies) you effectively remove pre-existing franchises and get a sense of how a new game with no pre-existing fan base sells. Also verified the assumption that no specific platform had guidelines regarding title length.

```{r, fig.width=6,fig.height=5}
# Finding Name length and total sales
name_length <- clean %>%
  mutate(total_sales = (NA_Sales+EU_Sales+JP_Sales+Other_Sales))%>%
  group_by(Name,Platform, total_sales) %>%
  tally() %>%
  mutate(chr_length = nchar(Name))

# Creating table
name_length %>%
  ungroup() %>%
  select(c("Name", "chr_length")) %>%
  head(26) %>%
  knitr::kable(caption = "Table _: The number of characters in each title")

              
# Revealing any relationship between title length and sales
ggplot(filter(name_length, total_sales < 5), aes(x = chr_length, y = total_sales)) +
  geom_point(aes(colour = Platform), alpha=0.2, show.legend = FALSE) +
  geom_smooth(se=FALSE)+
  labs(title = str_wrap("Figure 7: The amount of sales observed compared to the length of the title for games that sold less than 5 million copies", 70),
       x = "Number of characters in title",
       y = "Total sales (in millions)") +
  theme_minimal()
```

**Publisher** - Although this variable was not a predictor itself, it is worthwhile investigating for trends that might provide some inspiration for the model building phase. For instance, some Publishers (such as 'Nintendo') are also the producers of the console. The name alone does not give an indication of how well established the company is nor the fan base they have in terms of market share. We can get an estimate of this if we instead consider the average sales per game for each Publisher. Below I've prepared a plot to approximate the average proportion of market share each Publisher has:

```{r, fig.width=6,fig.height=5}
# Finding the total sales for each Publisher
clean$Publisher <- as.factor(clean$Publisher)
established <- clean %>%
  mutate(publisher_sales = (NA_Sales+EU_Sales+JP_Sales+Other_Sales),
         releases = 1) %>%
  group_by(Publisher, publisher_sales, releases) %>%
  tally() %>%
  group_by(Publisher) %>%
  summarise(releases = sum(releases),
            publisher_sales = sum(publisher_sales)) %>%
  mutate(estimated_market_share = ((publisher_sales/releases)/sum(publisher_sales)))

# Creating table
established %>%
  ungroup() %>%
  select(c("Publisher", "publisher_sales")) %>%
  arrange(desc(publisher_sales) ) %>%
  head(100) %>%
  knitr::kable(caption = "Table _:The top 100 Publishers in terms of sales in millions")

# Plotting the Publishers that produced more than 20 million sales
ggplot(filter(established, estimated_market_share > 0.0001),
       aes( x = estimated_market_share, y = fct_reorder(Publisher, estimated_market_share))) +
  geom_point() +
  ggtitle(str_wrap(" Figure 8: Using proportion of average sales per game for each publisher to signify brand establishment, top 50 Publishers", 70)) +
  ylab("Publishers") +
  xlab("Proportion of Average Sales per Game") +
  theme_minimal()

# Finding the amount of sales per Platform over time
meaningful_platform <- clean %>%
  mutate(total_sales = (NA_Sales+EU_Sales+JP_Sales+Other_Sales))%>%
  group_by(Platform, Year, total_sales) %>%
  tally() %>%
  summarise(platform_sales_for_year = sum(total_sales)) 

# Creating table
clean %>%
  mutate(total_sales = (NA_Sales+EU_Sales+JP_Sales+Other_Sales))%>%
  group_by(Platform, total_sales) %>%
  tally() %>%
  summarise(platform_sales_for_year = sum(total_sales)) %>%
  arrange(desc(platform_sales_for_year)) %>%
  knitr::kable(caption = "Table _: Sales made for each category of platform")

  ```